/*
 ImageTo3DProcessor.swift
 Purpose: Convert reference images to 3D room models using Vision framework
 
 Features:
 - Edge detection and corner detection
 - Room boundary extraction
 - 3D model generation from 2D image
 - Uses Apple Vision framework (free, no external dependencies)
 
 Created: December 2025
 */

import Foundation
import Vision
import CoreGraphics
import CoreImage
import UIKit
import simd

/// Processes images to extract room data and generate 3D models
class ImageTo3DProcessor {
    
    // MARK: - Image Processing
    
    /// Analyze image and extract room boundaries
    static func analyzeRoomImage(_ image: UIImage) -> RoomFromImageData? {
        guard let cgImage = image.cgImage else {
            print("âŒ Failed to get CGImage from UIImage")
            return nil
        }
        
        print("ðŸ“· Analyzing room image...")
        
        // Detect edges in the image
        let edges = detectEdges(cgImage: cgImage)
        guard edges.count > 0 else {
            print("âš ï¸ No edges detected in image")
            return nil
        }
        
        print("âœ… Detected \(edges.count) edge points")
        
        // Find corner points (potential room corners)
        let corners = findCorners(edges: edges, imageSize: CGSize(width: cgImage.width, height: cgImage.height))
        print("ðŸ” Found \(corners.count) potential corners")
        
        // Extract room boundaries
        let boundingBox = extractBoundingBox(corners: corners, imageSize: CGSize(width: cgImage.width, height: cgImage.height))
        print("ðŸ“ Room bounding box: \(boundingBox)")
        
        // Estimate room dimensions based on perspective
        let estimatedDimensions = estimateRoomDimensions(
            boundingBox: boundingBox,
            imageSize: CGSize(width: cgImage.width, height: cgImage.height),
            corners: corners
        )
        
        print("ðŸ“ Estimated room dimensions: \(estimatedDimensions)")
        
        return RoomFromImageData(
            originalImage: image,
            detectedCorners: corners,
            boundingBox: boundingBox,
            estimatedWidth: estimatedDimensions.width,
            estimatedLength: estimatedDimensions.length,
            estimatedHeight: estimatedDimensions.height
        )
    }
    
    // MARK: - Edge Detection
    
    private static func detectEdges(cgImage: CGImage) -> [CGPoint] {
        var edges: [CGPoint] = []
        
        // Use CIFilter for edge detection (free, built-in)
        let inputImage = CIImage(cgImage: cgImage)
        
        // Apply Sobel edge detection
        let context = CIContext()
        let edgeDetectFilter = CIFilter(name: "CIEdges")
        edgeDetectFilter?.setValue(inputImage, forKey: kCIInputImageKey)
        edgeDetectFilter?.setValue(1.0, forKey: kCIInputIntensityKey)
        
        guard let edgeImage = edgeDetectFilter?.outputImage else {
            return edges
        }
        
        // Convert to CGImage for processing
        guard let cgEdgeImage = context.createCGImage(edgeImage, from: edgeImage.extent) else {
            return edges
        }
        
        // Extract edge points from image
        edges = extractEdgePoints(from: cgEdgeImage, threshold: 0.5)
        
        return edges
    }
    
    private static func extractEdgePoints(from cgImage: CGImage, threshold: Float) -> [CGPoint] {
        var points: [CGPoint] = []
        
        guard let dataProvider = cgImage.dataProvider,
              let data = dataProvider.data else {
            return points
        }
        
        let width = cgImage.width
        let height = cgImage.height
        let pixelData = CFDataGetBytePtr(data)
        
        // Sample points at intervals to avoid too many points
        let sampleInterval = 5
        
        for y in stride(from: 0, to: height, by: sampleInterval) {
            for x in stride(from: 0, to: width, by: sampleInterval) {
                let pixelIndex = (y * width + x) * 4
                
                // Check if pixel is an edge (high intensity)
                if pixelIndex + 3 < CFDataGetLength(data) {
                    let alpha = CGFloat(pixelData![pixelIndex + 3]) / 255.0
                    if alpha > CGFloat(threshold) {
                        points.append(CGPoint(x: x, y: y))
                    }
                }
            }
        }
        
        return points
    }
    
    // MARK: - Corner Detection
    
    private static func findCorners(edges: [CGPoint], imageSize: CGSize) -> [CGPoint] {
        guard edges.count > 10 else { return [] }
        
        var corners: [CGPoint] = []
        
        // Find extreme points (likely room corners)
        let minX = edges.min(by: { $0.x < $1.x })?.x ?? 0
        let maxX = edges.max(by: { $0.x < $1.x })?.x ?? imageSize.width
        let minY = edges.min(by: { $0.y < $1.y })?.y ?? 0
        let maxY = edges.max(by: { $0.y < $1.y })?.y ?? imageSize.height
        
        // Add corners at extremes (top-left, top-right, bottom-left, bottom-right)
        corners.append(CGPoint(x: minX, y: minY)) // Top-left
        corners.append(CGPoint(x: maxX, y: minY)) // Top-right
        corners.append(CGPoint(x: minX, y: maxY)) // Bottom-left
        corners.append(CGPoint(x: maxX, y: maxY)) // Bottom-right
        
        // Find additional corners by looking for significant direction changes in edges
        let clusteredCorners = clusterCorners(corners, radius: 50)
        
        return clusteredCorners
    }
    
    private static func clusterCorners(_ corners: [CGPoint], radius: CGFloat) -> [CGPoint] {
        var clusters: [[CGPoint]] = []
        var used = Set<Int>()
        
        for (i, corner) in corners.enumerated() {
            if used.contains(i) { continue }
            
            var cluster = [corner]
            used.insert(i)
            
            for (j, other) in corners.enumerated() {
                if used.contains(j) { continue }
                
                let distance = hypot(corner.x - other.x, corner.y - other.y)
                if distance < radius {
                    cluster.append(other)
                    used.insert(j)
                }
            }
            
            clusters.append(cluster)
        }
        
        // Return average of each cluster
        return clusters.map { cluster in
            let avgX = cluster.map { $0.x }.reduce(0, +) / CGFloat(cluster.count)
            let avgY = cluster.map { $0.y }.reduce(0, +) / CGFloat(cluster.count)
            return CGPoint(x: avgX, y: avgY)
        }
    }
    
    // MARK: - Bounding Box Extraction
    
    private static func extractBoundingBox(corners: [CGPoint], imageSize: CGSize) -> CGRect {
        guard !corners.isEmpty else {
            return CGRect(origin: .zero, size: imageSize)
        }
        
        let minX = corners.min(by: { $0.x < $1.x })?.x ?? 0
        let maxX = corners.max(by: { $0.x < $1.x })?.x ?? imageSize.width
        let minY = corners.min(by: { $0.y < $1.y })?.y ?? 0
        let maxY = corners.max(by: { $0.y < $1.y })?.y ?? imageSize.height
        
        return CGRect(
            x: minX,
            y: minY,
            width: maxX - minX,
            height: maxY - minY
        )
    }
    
    // MARK: - Room Dimension Estimation
    
    private static func estimateRoomDimensions(
        boundingBox: CGRect,
        imageSize: CGSize,
        corners: [CGPoint]
    ) -> (width: Float, length: Float, height: Float) {
        // Estimate based on perspective and image analysis
        
        // Assume standard ceiling height (2.7m)
        let estimatedHeight: Float = 2.7
        
        // Bounding box aspect ratio
        let aspectRatio = Float(boundingBox.width / boundingBox.height)
        
        // Estimate dimensions from bounding box (assume average room size)
        // Scale based on percentage of image covered
        let imageCoverage = Float((boundingBox.width * boundingBox.height) / (imageSize.width * imageSize.height))
        
        // Heuristic: larger coverage = smaller room, smaller coverage = larger room
        let scaleFactor = max(0.5, min(3.0, 1.0 / imageCoverage))
        
        // Default room size adjusted by scale factor
        var width = 4.0 * scaleFactor  // 4m default
        var length = 5.0 * scaleFactor  // 5m default
        
        // Adjust based on aspect ratio
        if aspectRatio > 1.0 {
            // Wider image = wider room
            width *= scaleFactor
            length *= (1.0 / scaleFactor)
        } else {
            // Taller image = longer room
            width *= (1.0 / scaleFactor)
            length *= scaleFactor
        }
        
        return (
            width: max(2.0, min(20.0, width)),      // Clamp between 2-20m
            length: max(2.0, min(20.0, length)),    // Clamp between 2-20m
            height: estimatedHeight
        )
    }
    
    // MARK: - 3D Model Generation
    
    static func generate3DModel(from roomData: RoomFromImageData) -> CapturedRoomSummary {
        print("ðŸŽ¨ Generating 3D room model from image analysis...")
        
        let roomId = "room-from-image-\(UUID().uuidString.prefix(8))"
        let floorArea = roomData.estimatedWidth * roomData.estimatedLength
        
        // Create walls from estimated dimensions
        let walls = createWallsFromDimensions(
            width: roomData.estimatedWidth,
            length: roomData.estimatedLength,
            height: roomData.estimatedHeight
        )
        
        // Detect potential openings (doors/windows) from image
        let openings: [OpeningSummary] = []  // Can be enhanced with door/window detection
        
        let summary = CapturedRoomSummary(
            roomId: roomId,
            floorArea: floorArea,
            walls: walls,
            openings: openings
        )
        
        print("âœ… Generated 3D model: \(walls.count) walls, \(openings.count) openings")
        
        return summary
    }
    
    private static func createWallsFromDimensions(
        width: Float,
        length: Float,
        height: Float
    ) -> [WallSummary] {
        var walls: [WallSummary] = []
        
        // Half dimensions for wall positioning
        let halfWidth = width / 2.0
        let halfLength = length / 2.0
        
        // Wall 1: Front (along X axis, positive Z)
        walls.append(WallSummary(
            id: "wall-front",
            height: height,
            thickness: 0.15,
            length: width,
            position: simd_float3(0, 0, halfLength),
            normal: simd_float3(0, 0, 1)
        ))
        
        // Wall 2: Back (along X axis, negative Z)
        walls.append(WallSummary(
            id: "wall-back",
            height: height,
            thickness: 0.15,
            length: width,
            position: simd_float3(0, 0, -halfLength),
            normal: simd_float3(0, 0, -1)
        ))
        
        // Wall 3: Left (along Z axis, negative X)
        walls.append(WallSummary(
            id: "wall-left",
            height: height,
            thickness: 0.15,
            length: length,
            position: simd_float3(-halfWidth, 0, 0),
            normal: simd_float3(-1, 0, 0)
        ))
        
        // Wall 4: Right (along Z axis, positive X)
        walls.append(WallSummary(
            id: "wall-right",
            height: height,
            thickness: 0.15,
            length: length,
            position: simd_float3(halfWidth, 0, 0),
            normal: simd_float3(1, 0, 0)
        ))
        
        return walls
    }
}

// MARK: - Data Models

/// Room data extracted from image analysis
struct RoomFromImageData {
    let originalImage: UIImage
    let detectedCorners: [CGPoint]
    let boundingBox: CGRect
    let estimatedWidth: Float
    let estimatedLength: Float
    let estimatedHeight: Float
}
